{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6237eb58",
   "metadata": {},
   "source": [
    "## Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede96fd5",
   "metadata": {},
   "source": [
    "There are essentially three parameters to tune in the polynomial regression class. The first, and the most obvious, is the polynomial order, which has the keyword ``order`` in the constructor. The next is the type of polynomial interaction terms called ``mindex_type``. \"total_orer\" os the default, but an alternative is \"hyperbolic\" which has even fewer interaction terms which emphasizes more higher-order terms. In practice, this rarely leads to a better polynomial, but we can try it anyway. Last, but not least, there is the polynomial ``fit_type``, which determines the solver used to solve the least squares problem (Note even though polynomials are non-linear, the fitting boils down to a linear problem). This can be a bunch of different algorithms, but the three most widely used are 'linear', 'LassoCV', and 'ElasticNetCV'. \n",
    "\n",
    "With these parameters in mind, we create a parameter grid just like one would when using the GridSearchCV method in sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "947fbc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pce_grid = {\n",
    "    'order': list(range(1,12)),\n",
    "    'mindex_type': ['total_order','hyperbolic'],\n",
    "    'fit_type': ['linear','ElasticNetCV','LassoCV'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2030bc53",
   "metadata": {},
   "source": [
    "Now we use the regression wrapper CV class which wraps the PCEReg class in sklearn's grid search CV functionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52590ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 66 candidates, totalling 330 fits\n",
      "Fitting 5 folds for each of 66 candidates, totalling 330 fits\n",
      "Hyper-parameter CV PCE score is 0.998\n"
     ]
    }
   ],
   "source": [
    "# hyper-parameter tune the PCE regression class using all available cores\n",
    "pce = tesuract.RegressionWrapperCV(\n",
    "    regressor='pce',\n",
    "    reg_params=pce_grid,\n",
    "    n_jobs=-1,\n",
    "    scorer='r2')\n",
    "pce.fit(X,y)\n",
    "print(\"Hyper-parameter CV PCE score is {0:.3f}\".format(pce.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c4b367",
   "metadata": {},
   "source": [
    "Why so many fits? For each k-fold (5 total) we have to compute 66 fits corresponding to 66 different parameter combinations. This repeats five times to get an average cross validation score. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09b90e1",
   "metadata": {},
   "source": [
    "Look at that! We got all the way to an R2 score of basically 1! How did we do that? One of our parameter combinations must have been really good. Which one was it? We can easily find out by called the best_params_ attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "729254db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_type': 'ElasticNetCV', 'mindex_type': 'total_order', 'order': 4}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pce.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80741f5c",
   "metadata": {},
   "source": [
    "So it seems like 8th order way too high and probably overfit, so a fourth order was much better. Elastic net regularization also seemed to work the best, which uses a mix of l1 and l2 regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bac3a89",
   "metadata": {},
   "source": [
    "Now, to be fair, we probably should hyper-parameter tune the MLP regressor to perform a completely fair comparison, and it may probably give us ultimately a better model. In general however, neural networks are much hard to hyper-parameter tune and take longer to train, so the polynomial model can be preferred when accuracy and simplicity is required. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01385175",
   "metadata": {},
   "source": [
    "## Feature importances\n",
    "\n",
    "One last thing, before we move onto more advanced usage cases, in particular, tensor surrogates. With orthogonal polynomials like in PCEs, we can (almost) automatically obtain feature importances in the form of Sobol sensitivity indices. In particular, we can call feature importances to obtain normalized (summed to one) Sobol total order indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "587f855a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25341252, 0.2553728 , 0.08742023, 0.31989295, 0.08390151])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtain the best estimator after hyper-parameter tuning\n",
    "pce_best = pce.best_estimator_\n",
    "# compute the normalized (sum to 1) Sobol total order indices\n",
    "pce_best.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf53116",
   "metadata": {},
   "source": [
    "Now technically, the Sobol total order indices shouldn't be normalized, but we do it for consistency and with only some loss of generality. For the original total order indices use ``computeSobol()`` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "944c0058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.27158784222563226,\n",
       " 0.27368872135122857,\n",
       " 0.09369020534664568,\n",
       " 0.34283640074485666,\n",
       " 0.0899191144641273]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pce_best.computeSobol()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d706d6",
   "metadata": {},
   "source": [
    "The larger the value, the more \"important\" the parameter is. So, the first, second and fourth parameters are more importance features in the model than the remaining two. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
